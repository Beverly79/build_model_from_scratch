{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079e4f70",
   "metadata": {},
   "source": [
    "https://jaketae.github.io/study/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d8e9b",
   "metadata": {},
   "source": [
    "### Objective/loss/cost function\n",
    "\n",
    "For each position $t = 1, ... , ùëá$, predict context words within a window of fixed size $m$, given center word $w_j$. Data likelihood:\n",
    "\n",
    "$$Likelihood = L(\\theta) = \\prod_{t}^{T} \\prod_{\\substack{-m \\leq j \\leq m \\\\ j \\neq m}} P(w_{t+j}|{w_{t}; \\theta}) \\tag{1}$$\n",
    "\n",
    "The objective/loss/cost function for $(1)$ is the average negative log likelihood: \n",
    "\n",
    "$$ L = - \\frac{1}{T} logL(\\theta) = - \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq m}} log P(w_{t+j}|{w_{t}; \\theta}) \\tag{2}$$\n",
    "\n",
    "\n",
    "### Prediction function\n",
    "\n",
    "Denote by $v_{c}$ and $v_{o}$ respectively the center word and the context word, using **softmax**, we get the following prediction function of predicting $v_{c}$ given $v_{o}$ and the vocubulary $V$:\n",
    "\n",
    "$$P(o|c) = \\frac{exp(\\mathbf{u_{o}^{T} v_{c}})}{\\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})} \\tag{3}$$\n",
    "\n",
    "\n",
    "### Forward Propagation \n",
    "\n",
    "$$ A_{1} = XW_{1}$$\n",
    "$$ A_{2} = A_{1} W_{2} $$\n",
    "$$Z = softmax(A_{2})$$\n",
    "\n",
    "### Back Propagation\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial A_{2}} = Z - y$$\n",
    "$$ \\frac{\\partial L}{\\partial W_{2}} = A_{1}^T \\left(Z - y\\right)$$\n",
    "$$ \\frac{\\partial L}{\\partial A_{1}} = \\left(Z - y\\right) W_{2}^T$$\n",
    "$$ \\frac{\\partial L}{\\partial W_{1}} = X^T \\left[ \\left(Z - y\\right) W_{2}^T \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a19dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f3c75",
   "metadata": {},
   "source": [
    "## prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9600c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc936ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade6a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a47474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(tokens):\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    for i, token in enumerate(set(tokens)): \n",
    "        # only count the same word once, id is the first time it showed up so we can save it to a dictonary\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "    \n",
    "    return word_to_id, id_to_word # return two dictionaries\n",
    "\n",
    "\n",
    "# example:  \n",
    "# text = \"repeat repeat repeat three times\"\n",
    "# tokens = ['repeat', 'repeat', 'repeat', 'three', 'times']\n",
    "# word_to_id = {'repeat': 0, 'three': 1, 'times': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5ddca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id, id_to_word = mapping(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f9a0dc",
   "metadata": {},
   "source": [
    "### Skip-gram\n",
    "predicting the context words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e132df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(*iterables):\n",
    "    for iterable in iterables:\n",
    "        yield from iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9692f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(id, vocab_size):\n",
    "    vector = [0] * vocab_size\n",
    "    vector[id] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be0ca1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    X = [] # gonna be a list of lists (each list is a one hot encoding vector of some word)\n",
    "    y = []\n",
    "    n_tokens = len(tokens)\n",
    "    \n",
    "    for i in range(n_tokens):\n",
    "        # i is the index of tokens\n",
    "        # idx gets the index from center word -m to +m position words\n",
    "        # for example: i = 2, m = 4, idx should be range(0,7), which is 0, 1, 2, 3, 4, 5, 6\n",
    "        idx = concat(\n",
    "            range(max(0, i - window_size), i), \n",
    "            range(i, min(n_tokens, i + window_size + 1))\n",
    "        )\n",
    "        for j in idx:\n",
    "            if i == j: # skip the center word itself\n",
    "                continue\n",
    "            # for example: i = 2, m = 4, X would append the word one hot encoding, and y would append one of the surrounding words one hot encoding\n",
    "            X.append(one_hot_encode(word_to_id[tokens[i]], len(word_to_id)))\n",
    "            y.append(one_hot_encode(word_to_id[tokens[j]], len(word_to_id)))\n",
    "\n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "670a0e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330, 60) (330, 60) True\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_training_data(tokens, word_to_id, 2)\n",
    "print(X.shape, y.shape, X.shape==y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a4bf8",
   "metadata": {},
   "source": [
    "## build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446b1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(vocab_size, n_embedding):\n",
    "    # generate initial random weight\n",
    "    model = {\n",
    "        \"w1\": np.random.randn(vocab_size, n_embedding), # input*w1 is the matrix of all the embedding vectors\n",
    "        \"w2\": np.random.randn(n_embedding, vocab_size)\n",
    "    }\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "648bcc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_network(len(word_to_id), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc85b697",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd6a383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    prob_vector = []\n",
    "    for x in X:\n",
    "        exp = np.exp(x) # for each scalar in this vector, calculate the weight of it's exp()\n",
    "        prob_vector.append(exp / exp.sum())\n",
    "    return prob_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44773dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, X, return_all_layer=True):\n",
    "    layer = {}\n",
    "    \n",
    "    layer[\"a1\"] = np.dot(X, model[\"w1\"]) # first layer is to convert input one hot encoding to embedding space through W1\n",
    "    layer[\"a2\"] = np.dot(layer[\"a1\"], model[\"w2\"]) \n",
    "    layer[\"z\"] = softmax(layer[\"a2\"]) # last layer is softmax to convert layer 2 to predict probability of vocalubary\n",
    "    \n",
    "    if not return_all_layer:\n",
    "        return layer[\"z\"] # just return the softmax layer\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "790f7dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dimension of layer 1 output, should be n-input * n-embedding-space\n",
    "np.dot(X, model[\"w1\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fcfca42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 60)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dimension of layer 2 output, should be n-input * n-vocabulary\n",
    "np.dot(np.dot(X, model[\"w1\"]), model[\"w2\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0296fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(z, y):\n",
    "    return - np.sum(np.log(z) * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d78b7746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(model, X, y, lr):\n",
    "    cache  = forward(model, X) # the temporary value of the prediction y\n",
    "    da2 = cache[\"z\"] - y # derivative of A2\n",
    "    dw2 = np.dot(cache[\"a1\"].T, da2)\n",
    "    da1 = np.dot(da2, model[\"w2\"].T)\n",
    "    dw1 = np.dot(X.T, da1)\n",
    "    assert(dw2.shape == model[\"w2\"].shape) # test shape\n",
    "    assert(dw1.shape == model[\"w1\"].shape)\n",
    "    model[\"w1\"] -= lr * dw1 # update W1 and W2\n",
    "    model[\"w2\"] -= lr * dw2\n",
    "    return cross_entropy(cache[\"z\"], y) # current prediction's loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03bf05f",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "131a5c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_iter = 50\n",
    "learning_rate = 0.05\n",
    "\n",
    "history = [backward(model, X, y, learning_rate) for _ in range(n_iter)]\n",
    "\n",
    "plt.plot(range(len(history)), history, color=\"skyblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f9c0fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine\n",
      "so\n",
      "are\n",
      "intelligence\n",
      "build\n",
      "algorithms\n",
      "the\n",
      "learning\n",
      "a\n",
      "do\n",
      "automatically\n",
      "of\n",
      "is\n",
      "subset\n",
      "through\n",
      "seen\n",
      "to\n",
      "as\n",
      "in\n",
      "that\n",
      "variety\n",
      "training\n",
      "used\n",
      "perform\n",
      "experience\n",
      "order\n",
      "improve\n",
      "study\n",
      "it\n",
      "artificial\n",
      "tasks\n",
      "such\n",
      "data\n",
      "needed\n",
      "wide\n",
      "model\n",
      "conventional\n",
      "being\n",
      "filtering\n",
      "based\n",
      "mathematical\n",
      "programmed\n",
      "known\n",
      "where\n",
      "applications\n",
      "difficult\n",
      "explicitly\n",
      "develop\n",
      "sample\n",
      "predictions\n",
      "vision\n",
      "on\n",
      "make\n",
      "or\n",
      "and\n",
      "decisions\n",
      "infeasible\n",
      "email\n",
      "without\n",
      "computer\n"
     ]
    }
   ],
   "source": [
    "learning = one_hot_encode(word_to_id[\"learning\"], len(word_to_id))\n",
    "result = forward(model, [learning], return_all_layer=False)[0]\n",
    "\n",
    "for word in (id_to_word[id] for id in np.argsort(result)[::-1]):\n",
    "    print(word) # print the probability of words desc with the word \"learning\" as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "792a9701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.2628424  -0.23199393  0.73341386  0.97447621 -0.41641794 -0.68095658\n",
      "   2.61265629  0.74214787 -0.05703892 -1.04526872]\n",
      " [ 0.28876479  0.72380286  0.41703572 -1.93696817 -0.82172046  0.41757945\n",
      "  -0.72200732  0.97196456 -1.68163859 -1.42754947]\n",
      " [ 0.74553496 -0.69552587 -0.36087355 -1.44234776 -1.25861167  1.99996755\n",
      "  -1.41716946  0.85665773 -0.21359055 -0.21230389]]\n"
     ]
    }
   ],
   "source": [
    "print(model[\"w1\"][:3]) # the embedding of the first three words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b346f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, word):\n",
    "    try:\n",
    "        idx = word_to_id[word]\n",
    "    except KeyError:\n",
    "        print(\"`word` not in corpus\")\n",
    "    one_hot = one_hot_encode(idx, len(word_to_id))\n",
    "    return forward(model, one_hot)[\"a1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8043bb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00491847, -1.84355269,  0.27607591, -0.93631093, -0.59224958,\n",
       "        0.19767631, -0.78768098, -1.25834848,  0.70914154, -0.08872889])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding(model, \"machine\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
